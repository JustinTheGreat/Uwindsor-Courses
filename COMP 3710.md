COMP 3710 - AI concepts
=====
#### Course Weights
Assignments: 30%
Project Phase 1(Feb 4th): 10%
Test 1 (Feb 12th): 15%
Test 2 (March 20th): 15%
Project Phase 2 (March 31st): 20%

#### Useful Links
[TBD]()
### Definitions


## Chapter 1: Intro to AI
#### The Turing Test
In 1950, Alan Turing proposed a test in which an interrogator would sit at a computer terminal and communicate through it with two other participants - one a human, the other a machine. If the interrogator could not tell which one of the participants was the machine, then the machine was deemed to be intelligent.

#### Foundations of AI
##### Philosophies
- Rationalism
- Dualism (body + mind/spirit)
- Materialism (Law of physics)
- Induction (General rules that are made due to repeated associations with items)

##### Mathematics
- Algorithm
- Incompleteness Thereom (The true statements that are undecidable)
- Computable
- Tractability (A problem is called intractable if the time required grows exponentially with the size of the instances)
- NP - completeness

##### Economics
- Decision Theory
- Game Theory
- Operations Research
- Markov Decision Process (Sequential Decision Problems)

Linguistics - NLP

### Chapter 2: Intelligent Agents
**Agents:** Anything that can be viewed as perceiving it's environment through *sensors* and acting upon it through *actuators*
**Actuators:** Something that controls/moves things around in a system
**Percept Sequence:** The sequence of inputs and it's history
**Rational Agent:** Does the right thing
**Performance Measure:** Evaluates the environmental states of an agent

#### Rationality
Rationality is measured based off 4 parameters:
- The performance method that describes the success criterion/criteria
- The agent's prior knowledge of the environment
- The actions that the agent can perform
- The agent's percept sequence to date

The **Ideal Rational Agent** is able to for *each possible percept sequence* be able to *select an action* that is able to *maximize it's performance measure* given the evidence provided by the *percept sequence* and whatever built-in knowledge the agent has

#### Rationality vs Omniscient
**Omniscient (Perfect) Agent:** Knows the outcome of it's actions and can act accordingly

*vs*

**Rational Agent:** Acts according to it's percept and knowledge to date to attempt to maximize performance - which sometimes leads to imperfection

*Note: Basically saying omniscient is god/seer and rational is everything else*

#### Task Environment (PEAS)
When we define a rational agent, we group it's properties under **PEAS** for problem specification in the environment:
**P**erformance
**E**nvironment
**A**ctuators
**S**ensors

##### Example Agent: Automated Taxi Driver
**Perfomance:** Safe, Fast, Legals, Comfortable, Maximizes Profit
**Environment:** Roads, Other Structures, Pedestrians, Customers
**Actuators:** Steering Wheel, Accelerator, Break, Signal, Horn
**Sensors:** Cameras, Sonar, Speedometer, GPS, Odometer

## Chapter 3: Properties of task environments & structures of agents

### Properties of Task Environment

#### Fundamentally Observable vs Partiall Observable
**Fully Observable:** When an agent is able to view the complete environment at all times
**Partially Observable:** When an enviornment can only partially be observed because of incomplete/unreliable data
**Unobservable:** If an agent has no sensors at all

#### Single Agent vs Multi Agent
A agent working by itself vs multiple agents working together
**Competitive Multiagent Environment:** Multiple Agents working against each other/teams vs teams
**Cooperative Multiagent Environemnt:** A single team of agents working together against a problem

#### Deterministic vs Stochastic
**Deterministic:** If the next state is determined by current state and the action executed by the agent
**Stochastic:** Has aspects which are not controlable by the agent, we have to use probability estimation

#### Episodic vs Sequential
**Sequential Environment:** Where any current decision can affect every future decision
**Episodic Task Environment:** Where agent's experiences is divided into *atomic episodes*. The next episode does not use experiences from previous episodes

#### Discrete vs Continuous
**Discrete:** A limited number of distinct ations
**Continuous:** A range of values (continuous)

#### Known vs Unkown
**Known:** Agent knows all the rules of the environment
**Unkown:** Agent doesn't know all the rules of the environment

#### Static vs Dynamic
If environment is changing while agent is deliberating (performing action), environment is dynamic, otherwise it's static

### Structures of Agents
**Agent Behaviour:** The actions performed after a given sequence of percepts
**Agent Program:** A plan made by an AI to implement the agent's functions (Mapping from percepts to actions)
**Architecture:** How the program runs physically

###Types of Agents
**Simple Reflex Agents:** Chooses actions based on current percept, ignoring history and future consequences - still keeps a memory
**Model Based Reflex Agents:** Uses internal history and a model percept to make decisions
**Goal Based Agents:** Has a goal to get to, uses this knowledge to achieve it
**Utility Based Agents:** Maps a state onto a number to measure "happiness", "success", etc.
**Learning Agents:** Able to learn from previous experiences and adapt autonomosly

## Chapter 4:
####Depth First Search
- Starts to go most bottom left, to right into queue