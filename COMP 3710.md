COMP 3710 - AI concepts
=====
#### Course Weights
Assignments: 30%
Project Phase 1(Feb 4th): 10%
Test 1 (Feb 12th): 15%
Test 2 (March 20th): 15%
Project Phase 2 (March 31st): 20%

#### Useful Links
[TBD]()
### Definitions


## Chapter 1: Intro to AI
#### The Turing Test
In 1950, Alan Turing proposed a test in which an interrogator would sit at a computer terminal and communicate through it with two other participants - one a human, the other a machine. If the interrogator could not tell which one of the participants was the machine, then the machine was deemed to be intelligent.

#### Foundations of AI
##### Philosophies
- Rationalism
- Dualism (body + mind/spirit)
- Materialism (Law of physics)
- Induction (General rules that are made due to repeated associations with items)

##### Mathematics
- Algorithm
- Incompleteness Thereom (The true statements that are undecidable)
- Computable
- Tractability (A problem is called intractable if the time required grows exponentially with the size of the instances)
- NP - completeness

##### Economics
- Decision Theory
- Game Theory
- Operations Research
- Markov Decision Process (Sequential Decision Problems)

Linguistics - NLP

### Chapter 2: Intelligent Agents
**Agents:** Anything that can be viewed as perceiving it's environment through *sensors* and acting upon it through *actuators*
**Actuators:** Something that controls/moves things around in a system
**Percept Sequence:** The sequence of inputs and it's history
**Rational Agent:** Does the right thing
**Performance Measure:** Evaluates the environmental states of an agent

#### Rationality
Rationality is measured based off 4 parameters:
- The performance method that describes the success criterion/criteria
- The agent's prior knowledge of the environment
- The actions that the agent can perform
- The agent's percept sequence to date

The **Ideal Rational Agent** is able to for *each possible percept sequence* be able to *select an action* that is able to *maximize it's performance measure* given the evidence provided by the *percept sequence* and whatever built-in knowledge the agent has

#### Rationality vs Omniscient
**Omniscient (Perfect) Agent:** Knows the outcome of it's actions and can act accordingly

*vs*

**Rational Agent:** Acts according to it's percept and knowledge to date to attempt to maximize performance - which sometimes leads to imperfection

*Note: Basically saying omniscient is god/seer and rational is everything else*

#### Task Environment (PEAS)
When we define a rational agent, we group it's properties under **PEAS** for problem specification in the environment:
**P**erformance
**E**nvironment
**A**ctuators
**S**ensors

##### Example Agent: Automated Taxi Driver
**Perfomance:** Safe, Fast, Legals, Comfortable, Maximizes Profit
**Environment:** Roads, Other Structures, Pedestrians, Customers
**Actuators:** Steering Wheel, Accelerator, Break, Signal, Horn
**Sensors:** Cameras, Sonar, Speedometer, GPS, Odometer

## Chapter 3: Properties of task environments & structures of agents

### Properties of Task Environment

#### Fundamentally Observable vs Partiall Observable
**Fully Observable:** When an agent is able to view the complete environment at all times
**Partially Observable:** When an enviornment can only partially be observed because of incomplete/unreliable data
**Unobservable:** If an agent has no sensors at all

#### Single Agent vs Multi Agent
A agent working by itself vs multiple agents working together
**Competitive Multiagent Environment:** Multiple Agents working against each other/teams vs teams
**Cooperative Multiagent Environemnt:** A single team of agents working together against a problem

#### Deterministic vs Stochastic
**Deterministic:** If the next state is determined by current state and the action executed by the agent
**Stochastic:** Has aspects which are not controlable by the agent, we have to use probability estimation

#### Episodic vs Sequential
**Sequential Environment:** Where any current decision can affect every future decision
**Episodic Task Environment:** Where agent's experiences is divided into *atomic episodes*. The next episode does not use experiences from previous episodes

#### Discrete vs Continuous
**Discrete:** A limited number of distinct ations
**Continuous:** A range of values (continuous)

#### Known vs Unkown
**Known:** Agent knows all the rules of the environment
**Unkown:** Agent doesn't know all the rules of the environment

#### Static vs Dynamic
If environment is changing while agent is deliberating (performing action), environment is dynamic, otherwise it's static

### Structures of Agents
**Agent Behaviour:** The actions performed after a given sequence of percepts
**Agent Program:** A plan made by an AI to implement the agent's functions (Mapping from percepts to actions)
**Architecture:** How the program runs physically

### Types of Agents
**Simple Reflex Agents:** Chooses actions based on current percept, ignoring history and future consequences - still keeps a memory
**Model Based Reflex Agents:** Uses internal history and a model percept to make decisions
**Goal Based Agents:** Has a goal to get to, uses this knowledge to achieve it
**Utility Based Agents:** Maps a state onto a number to measure "happiness", "success", etc.
**Learning Agents:** Able to learn from previous experiences and adapt autonomosly

## Chapter 4: Problem Solving using Searching
### Formulating Problems
**Model:** A abstract mathematical concept
**Abstraction:** Removing detail from a representation
**Toy Problem:** Used to illustrate various problem-solving methods - compare algorithms
**Real-World Problem:** General Problem
### Algorithm Infrastructure
**Node:** Bookkeeping data structure for a tree
**State:** Configuration of the world
**Queue:** Data Structure to store the frontier

### Uninformed Search Strategy
- Also called blind search
- Strategies have no additional information about states beyond what is provided
- They can only generate successors and distinguish goal from non-goal states

**Informed State/Heurestic State:** Knows whether one goal state is more promising thatn the others

#### Search Algorithm Properties
Completeness: Is the algorithm guaranteed?
Optimality: Is the strategy optimal?
Time Complexity: How long does it take to find a solution?
Space Complexity: How much memory is needed?

#### Depth First Search
- Starts to go most bottom left, to right into queue

## Chapter 5: Informed Search
#### Heuristics Search
Heuristics/Informed Search exploits **additional Knowledge** for more promising paths
- General approach is **best first search**
- A node is selected based on an expansion strategy f(n) where f is the choice of stategy
- f is denoted a heuristic function h(n)
- If n is goal, h(n) = 0
#### Greedy best-first search
- Uses heuristic info to guide it's search
- Tries to expand the node that is closest to it's goal
- Evaluates nodes based off f(n) = h(n)
#### A* Algorithm
- Most widely known form of best-first search
- Expand fewer nodes by considering cost of reaching goal from node in each list
- Combination of UCS & Greedy

f(n)= g(n) + h(n)
f(n) = estimated cost of cheapest solution through n
g(n) = path cost from start node to node n
h(n) = estimated cost of the cheapest path from n to the goal

**Uniform Cost** orders by path cost, or backward cost g(n)  gives the path cost from start node to node *n*

**Greedy** orders by goal proximity, or forwards cost h(n) - the cost of cheapest path from n to the goal

#### Admissible Heuristics
- To be optimal, h(n) must be an admissable heuristic
- Admissible heuristic is one that never overestimates the cost to reach it's goal
- A* tree search is optimal is h(n) is admissible
- A* graph search is optimal if h(n) is consistent

## Chapter 6: Uninformed Search
**Breadth First Search:** Each root node is expanded first, then it's successors are expanded
**Depth First Search:** Expand the deepest node first
**Depth Limited Search:** DFS but with level limitation to avoid infinite state spaces
**Iterative Deepening Depth First Search:** Combination with depth first-search to find the best depth limit
**Uniform Cost-search:** Expands nodes in order for their optimal path cost
**Bi-directional Search:** Searches 2 directions simultaneously - one from the initial state and one backwards to the goal in hopes of reaching the middle. Checks if 2 frontiers meet.
**

## Chapter 7: Informed Search Strategies
### Hill - Climbing Search
Is the loop that continually moves in the direction of increasing value. It terminates when it reaches it's peak with no neighbour of higher value
- Sometimes reaches local maximum
- Does not use a search tree
- Does not look ahead to immediate neighbours

#### Drawbacks of Hill-climbing
**Local Maxima:** Peaks that aren't the highest point in the space.
**Rides:** A result in a sequence of local maxima that is difficult for greedy algorithms to navigate
**Plateaux:** a pleateau is a flat area of state-space landscape. It can be a flat local maximum where uphills exist.

#### Variants
**Stochastic Hill - Climbing:** Randomly chooses among available
**Steepest Ascent Hill - Climbing:**
**Random Restart Hill - Climbing:**

**Solutions:** 
### Local Beam Search
Keeps track of *k* states (based on beam value)
In each iteration
- Generates all successors of all k states
- Only retain the best k successors among them
- Can suffer from lack of diversity - they can quickly become concentrated in a small region of the state space
**Stochastic Variant:** 
**Major Difference:** Information is shared among the parallel k search threads

### Simulated Annealing
Based off the annealing process in materials processing when a metal cools and freezes into a crystalline state with minimal energy
**Hill Climbing:** Never makes "downhill" moves
**Random Walk:** Moves to a random successor
**Simulated Annealing:** An algorithm that combines the Hill Climbing & Random Walk.
